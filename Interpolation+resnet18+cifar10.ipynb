{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QAuIgQ9uHQij"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Zrxzual-1kE",
        "outputId": "5a2d7d43-6d2f-484e-c44f-d4366959e271"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AmDeoS93M6wc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eccdc797-45af-4b32-b614-47e726802d3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb -qU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "HZJtxoNhyfj3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RSIu8_WgYmv1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf8e8b85-4b45-40e8-d314-524d63e72fe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4ftNWdcBSOhz"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision.datasets import CIFAR10\n",
        "import torchvision.transforms as T\n",
        "\n",
        "\n",
        "class CIFAR10Dataset(Dataset):\n",
        "    \"\"\"CIFAR10 dataset.\n",
        "\n",
        "    Feature images are automatically flattened.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    root : str\n",
        "        Directory where the actual data is located (or downloaded to).\n",
        "\n",
        "    train : bool\n",
        "        If True the training set is returned (60_000 samples). Otherwise\n",
        "        the validation set is returned (10_000 samples).\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    tv_dataset : CIFAR10\n",
        "        Instance of the torchvision `CIFAR10` dataset class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root, train=True, download=True):\n",
        "      if train:\n",
        "        transform = T.Compose([\n",
        "          T.Resize(256),\n",
        "          T.RandomHorizontalFlip(),\n",
        "          T.ToTensor(),\n",
        "          T.Normalize(mean=[0.485, 0.456, 0.486], std=[0.229 , 0.224, 0.225]),\n",
        "        ]) \n",
        "      else:\n",
        "          transform = T.Compose([\n",
        "              T.Resize(256),\n",
        "              T.ToTensor(),\n",
        "              T.Normalize(mean=[0.485, 0.456, 0.486], std=[0.229 , 0.224, 0.225]),\n",
        "        ]) \n",
        "      self.tv_dataset = CIFAR10(\n",
        "            root, \n",
        "            train=train,\n",
        "            download=download,\n",
        "            transform=transform,\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Get the length of the dataset.\"\"\"\n",
        "        return len(self.tv_dataset)\n",
        "\n",
        "    def __getitem__(self, ix):\n",
        "        \"\"\"Get a selected sample.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        ix : int\n",
        "            Index of the sample to get.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x : torch.Tensor\n",
        "            Flattened feature tensor of shape `(784,)`.\n",
        "\n",
        "        y : torch.Tensor\n",
        "            Scalar representing the ground truth label. Number between 0 and 9.\n",
        "        \"\"\"\n",
        "        return self.tv_dataset[ix]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hJ3MQRhWMGPK"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "# результаты исследований показали, что только у сверточных и линейных слоев по 2 или по 3(после обрезки)\n",
        "            # параметра поэтому их можно фильтровать по этому признаку. Второе наблюдение - глобал прунинг прунит\n",
        "            # именно те слои, которые ты в него добавляешь из модульлиста. Поэтому осталось\n",
        "            #  1) сделать module_list со всеми слоями\n",
        "            #  2) в форварде запустить все через этот module_list\n",
        "            #  3) чтобы обрезать достаточно добавить слои с 2-3 параметрами в state_dict в parameters_to_prune\n",
        "            #  4) чтобы скопировать веса достаточно пройтись по всем элементам модульлиста с 2-3 параметрами\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self, image_channels=3, num_classes=10):\n",
        "        super(ResNet18, self).__init__()\n",
        "        layer_list = []\n",
        "        ds_list = []\n",
        "        self.relu = nn.ReLU()\n",
        "        self.avgpool = nn.AvgPool2d(4)\n",
        "        self.bn16 = nn.BatchNorm2d(16)\n",
        "        self.bn32 = nn.BatchNorm2d(32)\n",
        "        self.bn64 = nn.BatchNorm2d(64)\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(image_channels, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        \n",
        "        number_of_channels = [16, 32, 64]\n",
        "        strides_list = [4, 4, 4]\n",
        "        num_blocks = 3\n",
        "        for planes, stride in zip(number_of_channels, strides_list):\n",
        "          strides = [stride] + [1, 1]\n",
        "          layers = []\n",
        "          for stride in strides:\n",
        "              layer_list.append(nn.Conv2d(self.in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False))\n",
        "              layer_list.append(nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False))\n",
        "              if stride != 1 or self.in_planes != planes:\n",
        "                  ds_list.append(nn.Conv2d(self.in_planes, planes, kernel_size=1, stride=stride, bias=False))\n",
        "              self.in_planes = planes\n",
        "        \n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "        self.module_list = nn.ModuleList(layer_list)\n",
        "        self.downsample_list = nn.ModuleList(ds_list)\n",
        "\n",
        "        self.ptp = []\n",
        "        for layer in self.module_list:\n",
        "          self.ptp.append((layer, \"weight\"))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn16(self.conv1(x)))\n",
        "\n",
        "        # out = self.layer1(out)\n",
        "        tmp = self.relu(self.bn16(self.module_list[0](out)))\n",
        "        tmp = self.bn16(self.module_list[1](tmp))\n",
        "        tmp += self.bn16(self.downsample_list[0](out))\n",
        "        out = self.relu(tmp)\n",
        "\n",
        "        tmp = self.relu(self.bn16(self.module_list[2](out)))\n",
        "        tmp = self.bn16(self.module_list[3](tmp))\n",
        "        tmp += out\n",
        "        out = self.relu(tmp)\n",
        "\n",
        "        tmp = self.relu(self.bn16(self.module_list[4](out)))\n",
        "        tmp = self.bn16(self.module_list[5](tmp))\n",
        "        tmp += out\n",
        "        out = self.relu(tmp)\n",
        "\n",
        "        # out = self.layer2(out)\n",
        "        tmp = self.relu(self.bn32(self.module_list[6](out)))\n",
        "        tmp = self.bn32(self.module_list[7](tmp))\n",
        "        tmp += self.bn32(self.downsample_list[1](out))\n",
        "        out = self.relu(tmp)\n",
        "\n",
        "        tmp = self.relu(self.bn32(self.module_list[8](out)))\n",
        "        tmp = self.bn32(self.module_list[9](tmp))\n",
        "        tmp += out\n",
        "        out = self.relu(tmp)\n",
        "\n",
        "        tmp = self.relu(self.bn32(self.module_list[10](out)))\n",
        "        tmp = self.bn32(self.module_list[11](tmp))\n",
        "        tmp += out\n",
        "        out = self.relu(tmp)\n",
        "\n",
        "        # out = self.layer3(out)\n",
        "        tmp = self.relu(self.bn64(self.module_list[12](out)))\n",
        "        tmp = self.bn64(self.module_list[13](tmp))\n",
        "        tmp += self.bn64(self.downsample_list[2](out))\n",
        "        out = self.relu(tmp)\n",
        "\n",
        "        tmp = self.relu(self.bn64(self.module_list[14](out)))\n",
        "        tmp = self.bn64(self.module_list[15](tmp))\n",
        "        tmp += out\n",
        "        out = self.relu(tmp)\n",
        "\n",
        "        tmp = self.relu(self.bn64(self.module_list[16](out)))\n",
        "        tmp = self.bn64(self.module_list[17](tmp))\n",
        "        tmp += out\n",
        "        out = self.relu(tmp)\n",
        "\n",
        "        # head\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def prune_net(net, prune_ratio=0.3, method=\"l1\"):\n",
        "    \"\"\"Prune the whole net.\n",
        "\n",
        "    Modifies the module in-place. We make an assumption that each\n",
        "    layer has the bias included.\n",
        "\n",
        "    Parameters#\n",
        "    ----------\n",
        "    net : net\n",
        "        ResNet18 instance.\n",
        "\n",
        "    prune_ratio : float\n",
        "        Number between 0 and 1 representing the percentage of weights\n",
        "        to prune.\n",
        "\n",
        "    method : str, {\"l1\", \"random\"}\n",
        "        Pruning method to use.\n",
        "    \"\"\"\n",
        "\n",
        "    parameters_to_prune = []\n",
        "    for layer in net.module_list:\n",
        "      parameters_to_prune.append((layer, \"weight\"))\n",
        "\n",
        "    prune.global_unstructured(parameters_to_prune,\n",
        "                          prune.L1Unstructured,\n",
        "                          amount=prune_ratio)\n",
        "    \n",
        "\n",
        "\n",
        "def check_pruned_layer(layer):\n",
        "    \"\"\"Check if a module was pruned.\n",
        "\n",
        "    We require both the bias and the weight to be pruned.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    linear : nn.Linear\n",
        "        Linear module containing a bias.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "        True if the model has been pruned.\n",
        "    \"\"\"\n",
        "    params = {param_name for param_name, _ in layer.named_parameters()}\n",
        "\n",
        "    return \"weight_orig\" in params \n",
        "\n",
        "\n",
        "def copy_weights_layer(layer_unpruned, layer_pruned):\n",
        "    assert check_pruned_layer(layer_pruned)\n",
        "    assert not check_pruned_layer(layer_unpruned)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        layer_pruned.weight_orig.copy_(layer_unpruned.weight)\n",
        "\n",
        "\n",
        "def copy_weights_net(net_unpruned, net_pruned):\n",
        "    zipped = zip(net_unpruned.module_list, net_pruned.module_list)\n",
        "\n",
        "    for layer_unpruned, layer_pruned in zipped:\n",
        "        copy_weights_layer(layer_unpruned, layer_pruned)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GNyi8d0nMNfy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import tqdm\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import wandb\n",
        "\n",
        "\n",
        "def loop_dataloader(dataloader):\n",
        "    \"\"\"Loop infinitely over a dataloader.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataloader : DataLoader\n",
        "        DataLoader streaming batches of samples.\n",
        "\n",
        "    Yields\n",
        "    ------\n",
        "    X_batch : torch.Tensor\n",
        "        Batch of features.\n",
        "\n",
        "    y_batch : torch.Tensor\n",
        "        Batch of predictions.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        for x in iter(dataloader):\n",
        "            yield x\n",
        "\n",
        "\n",
        "def train(model, dataloader_train, loss_inst, optimizer, max_iter=10_000,\n",
        "          dataloader_val=None, val_freq=500, scheduler=None):\n",
        "    \"\"\"Run the training loop.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        Neural network (in our case MLP).\n",
        "\n",
        "    dataloader_train : DataLoader\n",
        "        Dataloader yielding training samples.\n",
        "\n",
        "    loss_inst : callable\n",
        "        Computes the loss when called.\n",
        "\n",
        "    optimizer : torch.optim.Optimizer\n",
        "        Instance of an optimizer.\n",
        "\n",
        "    max_iter : int\n",
        "        The number of iterations we run the training for\n",
        "        (= number of graident descent steps).\n",
        "\n",
        "    dataloader_val : None or DataLoader\n",
        "        Dataloader yielding validation samples. If provided it will\n",
        "        also single to us that we want to track metrics.\n",
        "\n",
        "    val_freq : int\n",
        "        How often evaluation run.\n",
        "    \"\"\"\n",
        "    global device\n",
        "    iterable = loop_dataloader(dataloader_train)\n",
        "    iterable = tqdm.tqdm(iterable, total=max_iter)\n",
        "    it = 0\n",
        "    for X_batch, y_batch in iterable:\n",
        "        if it == max_iter:\n",
        "            break\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        logit_batch = model(X_batch)\n",
        "\n",
        "        loss = loss_inst(logit_batch, y_batch)\n",
        "        if dataloader_val is not None:\n",
        "            wandb.log({\"loss\": loss}, step=it)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if scheduler is not None:\n",
        "          scheduler.step()\n",
        "\n",
        "        if it % val_freq == 0 and dataloader_val is not None:\n",
        "            is_equal = []\n",
        "\n",
        "            for X_batch_val, y_batch_val in dataloader_val:\n",
        "                X_batch_val = X_batch_val.to(device)\n",
        "                y_batch_val = y_batch_val.to(device)\n",
        "                is_equal.append(\n",
        "                    model(X_batch_val).argmax(dim=-1) == y_batch_val\n",
        "                )\n",
        "\n",
        "            is_equal_t = torch.cat(is_equal)\n",
        "            acc = is_equal_t.sum() / len(is_equal_t)\n",
        "            wandb.log({\"accuracy_val\": acc}, step=it)\n",
        "\n",
        "        it += 1\n",
        "\n",
        "\n",
        "\n",
        "def experiment(MAX_ITERS, PRUNE_ITERS, PRUNE_RATIOS , RANDOM_STATES, SCHEDULE, LR, project_name):\n",
        "    args = {\"max-iter\" : 30000, \"batch-size\" : 64, \"prune-iter\" : 1,\n",
        "          \"prune-method\" : \"l1\", \"prune-ratio\" : 0.2, \"val-freq\" : 250, \"random-state\" : 1}\n",
        "    \n",
        "    \n",
        "    for max_iter, prune_iter, prune_ratio, random_state, schedule, lr in zip(MAX_ITERS, PRUNE_ITERS, PRUNE_RATIOS, RANDOM_STATES, SCHEDULE, LR):\n",
        "            \n",
        "            os.mkdir(\"/content/drive/MyDrive/trained_models/prune_ratio_\" + str(prune_ratio))\n",
        "            \n",
        "\n",
        "            args[\"max-iter\"] = max_iter\n",
        "            args[\"prune-iter\"] = prune_iter\n",
        "            args[\"prune-ratio\"] = prune_ratio\n",
        "            args[\"random-state\"] = random_state\n",
        "\n",
        "            wandb.init(\n",
        "                project=project_name,\n",
        "                entity=\"bspanfilov\",\n",
        "                config=args,\n",
        "            )\n",
        "            wandb.define_metric(\"accuracy_val\", summary=\"max\")\n",
        "\n",
        "            dataset_train = CIFAR10Dataset(\n",
        "                \"data\",\n",
        "                train=True,\n",
        "                download=True,\n",
        "            )\n",
        "            dataset_val = CIFAR10Dataset(\n",
        "                \"data\",\n",
        "                train=False,\n",
        "                download=True,\n",
        "            )\n",
        "\n",
        "            if args[\"random-state\"] is not None:\n",
        "                torch.manual_seed(args[\"random-state\"])\n",
        "\n",
        "            dataloader_train = DataLoader(\n",
        "                dataset_train, batch_size=args[\"batch-size\"], shuffle=True\n",
        "            )\n",
        "            dataloader_val = DataLoader(\n",
        "                dataset_val, batch_size=args[\"batch-size\"], shuffle=True\n",
        "            )\n",
        "\n",
        "            kwargs = dict(\n",
        "                image_channels=3,\n",
        "                num_classes=10,\n",
        "            )\n",
        "\n",
        "            net = ResNet18(**kwargs).to(device)\n",
        "\n",
        "            net_copy = ResNet18(**kwargs).to(device)\n",
        "            net_copy.load_state_dict(net.state_dict())\n",
        "\n",
        "            loss_inst = nn.CrossEntropyLoss()\n",
        "            optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
        "            if schedule:\n",
        "              scheduler = MultiStepLR(optimizer, milestones=[20000,25000], gamma=0.1)\n",
        "            else:\n",
        "              scheduler = None\n",
        "\n",
        "            # Train and prune loop\n",
        "            if args[\"prune-ratio\"] > 0:\n",
        "                per_round_prune_ratio = 1 - (1 - args[\"prune-ratio\"]) ** (1 / args[\"prune-iter\"])\n",
        "\n",
        "                per_round_max_iter = int(args[\"max-iter\"] / args[\"prune-iter\"])\n",
        "\n",
        "                for prune_it in range(args[\"prune-iter\"]):\n",
        "                    train(\n",
        "                        net,\n",
        "                        dataloader_train,\n",
        "                        loss_inst,\n",
        "                        optimizer,\n",
        "                        max_iter=per_round_max_iter,\n",
        "                    )\n",
        "                    prune_net(net, per_round_prune_ratio, method=args[\"prune-method\"])\n",
        "                    torch.save(net, \"/content/drive/MyDrive/trained_models/prune_ratio_\" + str(prune_ratio) + \"/schedule_\" + str(int(schedule)) + \"_prune_it_\" + str(prune_it))\n",
        "\n",
        "                    copy_weights_net(net_copy, net)\n",
        "\n",
        "            # Run actual training with a final pruned network\n",
        "            train(\n",
        "                net,\n",
        "                dataloader_train,\n",
        "                loss_inst,\n",
        "                optimizer,\n",
        "                max_iter=args[\"max-iter\"],\n",
        "                dataloader_val=dataloader_val,\n",
        "                val_freq=args[\"val-freq\"],\n",
        "                scheduler=scheduler,\n",
        "            )\n",
        "\n",
        "            torch.save(net, \"/content/drive/MyDrive/trained_models/prune_ratio_\" + str(prune_ratio) + \"/schedule_\" + str(int(schedule)) + \"_pruned\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "7ak7kyZY0uD8",
        "outputId": "0deafa20-d39d-463e-91c1-fd9c471b9df5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a9a1b9f74796>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mRANDOM_STATES\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_ITERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPRUNE_ITERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPRUNE_RATIOS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRANDOM_STATES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSCHEDULE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ResNet18 + CIFAR10\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'experiment' is not defined"
          ]
        }
      ],
      "source": [
        "# тут просто настройки, с которыми я как раз запускал эксперименты\n",
        "MAX_ITERS=[30000, 30000]\n",
        "PRUNE_ITERS=[5, 5]\n",
        "PRUNE_RATIOS=[0.9, 0.9]\n",
        "SCHEDULE = [False,True]\n",
        "LR = [0.03, 0.03]\n",
        "RANDOM_STATES=[2, 2]\n",
        "\n",
        "experiment(MAX_ITERS, PRUNE_ITERS, PRUNE_RATIOS, RANDOM_STATES, SCHEDULE, LR, \"ResNet18 + CIFAR10\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def interpolation(alpha: float, first: ResNet18, second: ResNet18) -> ResNet18:\n",
        "  kwargs = dict(\n",
        "      image_channels=3,\n",
        "      num_classes=10,\n",
        "  )\n",
        "\n",
        "  ans = ResNet18(**kwargs)\n",
        "\n",
        "  print(ans.module_list[0].weight[0][0])\n",
        "\n",
        "  ans.conv1.weight[0] = alpha * first.conv1.weight[0] + (1 - alpha) * second.conv1.weight[0]\n",
        "\n",
        "  for i in range(len(ans.module_list)):\n",
        "    ans.module_list[i].weight[0] = (alpha * (first.module_list[i].weight_orig[0] * first.module_list[i].weight_mask[0]) + \\\n",
        "                                (1. - alpha) * (second.module_list[i].weight_orig[0] * second.module_list[i].weight_mask[0]))\n",
        "\n",
        "  for i in range(len(ans.downsample_list)):\n",
        "    ans.downsample_list[0].weight[0] = alpha * first.downsample_list[0].weight[0] + (1 - alpha) * second.downsample_list[0].weight[0]\n",
        "\n",
        "  ans.linear.weight[0] = alpha * first.linear.weight[0] + (1 - alpha) * second.linear.weight[0]\n",
        "\n",
        "  return ans\n"
      ],
      "metadata": {
        "id": "8oQcTGSi4YQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIiIFLlkHQCa"
      },
      "outputs": [],
      "source": []
    }
  ]
}